{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UHiGCDwbCyUS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YJ5lKN-Ots1",
        "outputId": "1ba36297-3f97-4167-f0cf-8b5c0ea918cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.array([0.485,0.456,0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])"
      ],
      "metadata": {
        "id": "ePNoGMWaOzJu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data transfromations\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean,std)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean,std)\n",
        "\n",
        "    ])\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "FH4xdQZTPAF7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQKz8iKeSv3G",
        "outputId": "0497c100-d8a1-4542-e1d5-44ac9bb6b1dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir= '/content/drive/MyDrive/data/hymenoptera_data'\n",
        "sets = ['train','val']\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x]) for x in ['train','val']}"
      ],
      "metadata": {
        "id": "mbE5lG7kSpX9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0)\n",
        " for x in ['train','val']}"
      ],
      "metadata": {
        "id": "sgsb4ud4TaUv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train','val']}\n",
        "dataset_sizes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46Qd-33ATtvV",
        "outputId": "d80c2732-fa3b-4d36-cbf9-8384c061fedf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': 246, 'val': 153}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = image_datasets['train'].classes\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGCaUHXcT_-Z",
        "outputId": "1f3b192d-4e5b-4f03-b331-bb3389db0afc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ants', 'bees']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,criterion, optimizer, scheduler, num_epochs=25):\n",
        "  since = time.time()\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f'epoch {epoch}/{num_epochs-1}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    # each epoch has a training and validation phase\n",
        "    for phase in ['train','val']:\n",
        "      if phase=='train':\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0.0\n",
        "\n",
        "      # iterate over data\n",
        "      for inputs,labels in dataloaders[phase]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.set_grad_enabled(phase=='train'):\n",
        "          outputs = model(inputs)\n",
        "          _,preds = torch.max(outputs,1)\n",
        "          loss = criterion(outputs,labels)\n",
        "\n",
        "          # backward\n",
        "          if phase== 'train':\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "      if phase == 'train':\n",
        "        scheduler.step()\n",
        "\n",
        "      epoch_loss = running_loss / dataset_sizes[phase]\n",
        "      epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "      print(f'{phase} loss: { epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "      # deep copy the model\n",
        "      if phase == 'val' and epoch_acc > best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print()\n",
        "\n",
        "  time_elapsed = time.time()- since\n",
        "  print(f'training completed in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s')\n",
        "  print(f'best val accuracy: {best_acc}')\n",
        "\n",
        "  #load the best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o59klV1fUOTo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= models.resnet18(pretrained = True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M8kaeoyYqXT",
        "outputId": "2e6a79d3-c225-4c58-ded8-a6e476771e74"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.001)\n",
        "criterion.to(device)\n",
        "\n",
        "# scheduler that can update the learning rate\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "sbngN4ZeZMJl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vppMwK2pZ2SH",
        "outputId": "1b64dead-9455-455c-b1e2-f86ad9cc7c2e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0/19\n",
            "----------\n",
            "train loss: 0.6461 Acc: 0.6016\n",
            "val loss: 0.5088 Acc: 0.7778\n",
            "\n",
            "epoch 1/19\n",
            "----------\n",
            "train loss: 0.4742 Acc: 0.7886\n",
            "val loss: 0.3541 Acc: 0.9020\n",
            "\n",
            "epoch 2/19\n",
            "----------\n",
            "train loss: 0.4719 Acc: 0.7724\n",
            "val loss: 0.2930 Acc: 0.8954\n",
            "\n",
            "epoch 3/19\n",
            "----------\n",
            "train loss: 0.3792 Acc: 0.8537\n",
            "val loss: 0.2579 Acc: 0.9085\n",
            "\n",
            "epoch 4/19\n",
            "----------\n",
            "train loss: 0.4082 Acc: 0.8374\n",
            "val loss: 0.2415 Acc: 0.9216\n",
            "\n",
            "epoch 5/19\n",
            "----------\n",
            "train loss: 0.3862 Acc: 0.8252\n",
            "val loss: 0.2313 Acc: 0.9216\n",
            "\n",
            "epoch 6/19\n",
            "----------\n",
            "train loss: 0.3544 Acc: 0.8374\n",
            "val loss: 0.2222 Acc: 0.9281\n",
            "\n",
            "epoch 7/19\n",
            "----------\n",
            "train loss: 0.3216 Acc: 0.8618\n",
            "val loss: 0.2170 Acc: 0.9281\n",
            "\n",
            "epoch 8/19\n",
            "----------\n",
            "train loss: 0.4520 Acc: 0.7724\n",
            "val loss: 0.2267 Acc: 0.9085\n",
            "\n",
            "epoch 9/19\n",
            "----------\n",
            "train loss: 0.3761 Acc: 0.8374\n",
            "val loss: 0.2428 Acc: 0.9150\n",
            "\n",
            "epoch 10/19\n",
            "----------\n",
            "train loss: 0.3602 Acc: 0.8496\n",
            "val loss: 0.2108 Acc: 0.9412\n",
            "\n",
            "epoch 11/19\n",
            "----------\n",
            "train loss: 0.4076 Acc: 0.8455\n",
            "val loss: 0.2186 Acc: 0.9281\n",
            "\n",
            "epoch 12/19\n",
            "----------\n",
            "train loss: 0.3606 Acc: 0.8049\n",
            "val loss: 0.2415 Acc: 0.9085\n",
            "\n",
            "epoch 13/19\n",
            "----------\n",
            "train loss: 0.3441 Acc: 0.8659\n",
            "val loss: 0.2143 Acc: 0.9150\n",
            "\n",
            "epoch 14/19\n",
            "----------\n",
            "train loss: 0.3712 Acc: 0.8374\n",
            "val loss: 0.2201 Acc: 0.9216\n",
            "\n",
            "epoch 15/19\n",
            "----------\n",
            "train loss: 0.3182 Acc: 0.8699\n",
            "val loss: 0.2141 Acc: 0.9346\n",
            "\n",
            "epoch 16/19\n",
            "----------\n",
            "train loss: 0.3293 Acc: 0.8618\n",
            "val loss: 0.2245 Acc: 0.9281\n",
            "\n",
            "epoch 17/19\n",
            "----------\n",
            "train loss: 0.2824 Acc: 0.8699\n",
            "val loss: 0.2353 Acc: 0.9085\n",
            "\n",
            "epoch 18/19\n",
            "----------\n",
            "train loss: 0.3037 Acc: 0.8618\n",
            "val loss: 0.2209 Acc: 0.9281\n",
            "\n",
            "epoch 19/19\n",
            "----------\n",
            "train loss: 0.3424 Acc: 0.8780\n",
            "val loss: 0.2415 Acc: 0.9085\n",
            "\n",
            "training completed in 4m 50s\n",
            "best val accuracy: 0.9411764705882353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IxG-dVx5b1zw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}